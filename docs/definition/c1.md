# Criterion 1 — Human Governance

## Definition

All critical decisions must be **supervised, validated, or overridable** by authorized human operators.

_High-impact actions require **dual-control**, which may combine direct human review with an AI-based or rule-based audit operating under explicit, inspectable policies._

_Human oversight also extends to incident response and recovery processes initiated under [**Criterion 9**](/definition/c9), ensuring that containment and subsequent knowledge-update actions remain under declared human authority._

## Purpose

To ensure that ultimate accountability for system behavior rests with humans rather than autonomous modules.

Human governance establishes two complementary domains of control:

1.  **Operational Supervision:** Direct or dual-control review of AI reasoning and outputs before high-impact execution.
2.  **Assurance Supervision:** Validation of system reflexes such as containment, rollback, and knowledge evolution when anomalies are detected ([**Criterion 9**](/definition/c9)).

This dual mandate guarantees that the orchestration fabric cannot act or self-correct outside explicit human consent, regardless of the number or autonomy of participating AI systems.

## 1. Dual-Control Explained

**Dual-control** means that no single actor — Human or AI — can unilaterally execute a high-impact decision. Two or more independent approvals are required, one of which must always be human.

#### Forms of Dual-Control

| Mode                                  | Participants                                                                  | Description                                                                                                                            |
| :------------------------------------ | :---------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------- |
| **Human + Human**                     | Two authorized reviewers                                                      | Classic double-sign-off; each reviews and approves independently.                                                                      |
| **Human + AI Audit (Indirect Audit)** | Human reviewer plus an AI or rule-based auditor                               | Human confirms context and intent; the AI auditor verifies compliance and technical correctness according to pre-declared audit rules. |
| **AI Audit + Human Override**         | Automated audit executes first; human reviewer confirms or vetoes the outcome | Efficient for high-volume, low-variance workflows (e.g., data-quality or bias scans).                                                  |

## 2. Direct vs Indirect Audit

| Type               | Executor                      | Nature                                                            | Example                                                                  |
| :----------------- | :---------------------------- | :---------------------------------------------------------------- | :----------------------------------------------------------------------- |
| **Direct Audit**   | Human                         | Manual review of AI decision or output.                           | Clinician reviews generated diagnosis report.                            |
| **Indirect Audit** | AI Auditor (rule set / model) | Automated review of the decision process against explicit policy. | Rule engine checks that all dosage values match the medication database. |

> An **AI Auditor** is permitted only when its auditing logic is **policy-declared, symbolically defined, and fully traceable.**

## 3. Trust Requirements for Indirect Audit

| Requirement                              | Linked Criterion                                                                                                                     | Explanation                                                                                                  |
| :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------- |
| **Declared Policy Basis**                | [**Criterion 2** — Policy-Enforced Operation](/definition/c2)                                                                        | Audit logic must originate from explicit, machine-readable policies.                                         |
| **Semantic and Provenance Traceability** | [**Criterion 3** — Interoperable Modularity](/definition/c3) **&** [**Criterion 8** — Transparency & Explainability](/definition/c8) | Audit results must be expressed as symbolic envelopes with evidence hashes, anchored for later verification. |
| **Explainable Verdicts**                 | [**Criterion 7** — Transparency & Explainability](/definition/c7)                                                                    | Each audit outcome must include a human-readable rationale.                                                  |

## 4. Outcome

[**Criterion 9**](/definition/c9) ensures that trust in Orchestration AI is **continuous, reflexive, and auditable**.

- It integrates detection, containment, and learning into a single governed process anchored in provenance.
- Every anomaly triggers a measurable reflex — **detect → assess → contain → evolve → certify → report** — closing the knowledge gap between problem and correction.
- Through the **Knowledge Assurance Dashboard**, humans remain in command of each step, while the system maintains uninterrupted safety, traceability, and improvement.

Provenance, Observability & Risk Containment thus operate as the **epistemic reflex centre** of Orchestration AI—proving what has happened, ensuring what is happening remains safe, and guaranteeing that what will happen next is better informed and fully governed.
