# Criterion 5 — Symbolic–Subsymbolic Integration

## Definition

Every subsymbolic (data-driven) module must be mediated, validated, and interpreted through **symbolic reasoning layers** that preserve explainability, consistency, and policy alignment under the supervision of the **[Control-Plane (C0)](/definition/c0)**, which governs mediation between reasoning styles.

_Symbolic knowledge constrains and explains statistical learning; statistical learning enriches and contextualizes symbolic reasoning._

## Purpose

To make sure reasoning and learning remain interpretable and governable.

- Machine-learning models generate powerful correlations but little meaning; symbolic systems reason precisely but lack adaptability.
- **Criterion 4** binds them into a **hybrid cognitive loop**—each compensating for the other’s weaknesses, both accountable to policy.

## 1. Key Concepts

| Concept                   | Description                                                                                                   |
| :------------------------ | :------------------------------------------------------------------------------------------------------------ |
| **Subsymbolic Reasoning** | Pattern-recognition through numeric models (LLMs, neural networks, vector retrieval, diffusion models).       |
| **Symbolic Reasoning**    | Logic-based inference through rules, graphs, or ontologies (RDR, CBR, knowledge graphs, constraint solvers).  |
| **Mediation Layer**       | The architectural bridge that translates numeric outputs into symbolic structures (and sometimes vice-versa). |
| **Semantic Alignment**    | Ensures that features, predictions, and outputs correspond to defined symbols and ontological terms.          |
| **Explainability Bridge** | Produces structured rationales (“claim–evidence–rule path”) from subsymbolic results.                         |

## 2. Problem It Solves

| Without C5                                                           | With C5                                                                  |
| :------------------------------------------------------------------- | :----------------------------------------------------------------------- |
| Models generate opaque probabilities or text; no verifiable meaning. | Outputs converted to symbolic claims with evidence and uncertainty.      |
| Rules cannot learn from data or adapt to drift.                      | Symbolic rules updated incrementally from observed statistical patterns. |
| Policy enforcement can’t reach inside ML reasoning.                  | Policies expressed as symbolic constraints applied to model behavior.    |

## 3. Operational Flow

**Task request**

↓

**Execution occurs within the Control-Plane’s Knowledge Mediation Layer (C0).**

↓

**Policy Engine attaches declared policies**

↓

**Knowledge Mediation Layer:**

- Map symbolic context → model parameters/features/prompts

- Inject policy-derived constraints (masks, thresholds, priors)

↓

**Subsymbolic Module executes (LLM, classifier, retriever)**

↓

**Knowledge Mediation Layer:**

- Parse raw output → structured symbolic claims

- Validate against rules, ontology, and policy

- Add uncertainty + rationale

↓

**Semantic Envelope → onward to next module / ledger**

## 4. Forms of Integration

| Form                            | Mechanism                                                                            | Example                                                            |
| :------------------------------ | :----------------------------------------------------------------------------------- | :----------------------------------------------------------------- |
| **Pre-Constraint Integration**  | Symbolic policies restrict input space or model configuration.                       | “Exclude patients < 18 years” → filter dataset / retrieval scope.  |
| **In-Process Integration**      | Symbolic constraints embedded in model reasoning (soft logic, constrained decoding). | Rule-guided beam search; differentiable logic layers.              |
| **Post-Validation Integration** | Symbolic reasoning validates or corrects model output.                               | RDR rule engine fixes inconsistent diagnoses.                      |
| **Bidirectional Learning**      | Data patterns propose symbolic rule updates under governance.                        | Discover correlation → propose new rule candidate → HITL approval. |

### 5. Relation to Other Criteria

| Related Criterion                                                       | Interaction                                                                               |
| :---------------------------------------------------------------------- | :---------------------------------------------------------------------------------------- |
| **[C0 — Control-Plane](/definition/c0)**                                | Hosts the Knowledge Mediation Layer and Semantic Adapters that execute integration.       |
| **[C1 — Human Governance](/definition/c1)**                             | Humans review and approve symbolic rules and hybrid updates.                              |
| **[C2 — Policy-Enforced Operation](/definition/c2)**                    | Symbolic rules express and enforce policies inside numeric reasoning.                     |
| **[C3 — Interoperable Modularity](/definition/c3)**                     | Defines standard interfaces so symbolic and data-driven modules interoperate.             |
| **[C4 — Semantic Communication Integrity](/definition/c4)**             | Provides the formal language used for translation and validation.                         |
| **[C6 — Epistemic Prudence](/definition/c6)**                           | Detects when hybrid reasoning becomes uncertain or unjustified and triggers abstain/HITL. |
| **[C7 — Incremental Knowledge Evolution](/definition/c7)**              | Allows symbolic knowledge or model weights to evolve under controlled feedback.           |
| **[C8 — Transparency & Explainability](/definition/c8)**                | Symbolic reasoning provides human-readable rationales.                                    |
| **[C9 — Provenance, Observability & Risk Containment](/definition/c9)** | Enables detection of model drift or rule inconsistency.                                   |
| **[C10 — Lifecycle Accountability](/definition/c10)**                   | Tracks versions of rules and models across hybrid updates.                                |

### 6. Human and LLM Roles

| Actor                         | Function                                                                                                   |
| :---------------------------- | :--------------------------------------------------------------------------------------------------------- |
| **Knowledge Engineer / HITL** | Approves new rules, interprets symbolic rationales, supervises adaptive updates.                           |
| **LLM**                       | Converts human natural language ↔ controlled symbolic representations, explains hybrid reasoning to users. |

Within the Human-in-the-Loop (HITL) interface, all module reasoning is symbolised into CSL. When human comprehension is required, the CSL is translated into Controlled Natural Language (CNL). Human inputs are reciprocally parsed from NL → CNL → CSL before reintegration. If the validator or recipient is a machine, CSL is used directly without natural-language conversion.

### 7. Outcome

[Criterion 4](/definition/c4) turns Orchestration AI from a collection of black boxes into a **hybrid reasoning system**:

- Symbolic logic ensures **policy compliance, structure, and explainability**.
- Subsymbolic models provide **adaptivity, perception, and generalization**.
- The Knowledge Mediation Layer fuses them into a single, transparent reasoning process.

**Symbolic layers keep the AIs honest; subsymbolic layers keep them smart.** Together they form the epistemic core of Trustworthy Orchestration AI.

**Symbolic–Subsymbolic Integration** unites data-driven adaptivity with policy-bound reasoning. Within the **[Control-Plane (C0)](/definition/c0)**, symbolic layers enforce structure and explainability (**[C2](/definition/c2)**, **[C4](/definition/c4)**, **[C8](/definition/c8)**), while subsymbolic models contribute perception and learning (**[C7](/definition/c7)**). The Human-in-the-Loop (HITL) interface operates as a symbolic governance module whose internal Controlled Natural Language (CNL) function mediates human communication into and from the Controlled Semantic Language (CSL). **[Epistemic Prudence (C6)](/definition/c6)** monitors coherence, and **[Provenance (C9)](/definition/c9)** anchors every hybrid decision for accountable audit.
